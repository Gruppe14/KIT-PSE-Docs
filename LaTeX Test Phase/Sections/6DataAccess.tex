\section{Data access}

\subsection{Tables}

Instead of creating tables by hand for every configuration file, functions
were added to make it possible to create tables dynamically from
a configuration. This depended on another feature,
storing the configuration files for which the tables had already been created.

\paragraph{Change detail}
\begin{itemize}
  \item Add feature to create tables dynamically
  \item Add feature to store and check if tables for configuration are already created
\end{itemize}

There showed up problems with the time and location table
<<<<<<< HEAD
which are always the same. Therefore the creating query
was changed to \texttt{CREATE IF NOT EXITS}.

Below are the creation queries for the SkyServer tables.
TODO codebeispiel
%TODO

\paragraph{Change detail}
\begin{itemize}
  \item Change table creation query to allow same tables for more configuration
\end{itemize}

%todo, look at
If dimensions of different configurations have the same name - except time and location - 
incorrect work will be caused. The table names are created from the dimension names.
It was decided to give the dimensions always a new name. If this is
not wanted, it could be changed by concatenating
the configuration name with dimension name and the dimension table short
to create the table name.

\subsection{Uploading}
\subsubsection{Query trunks}
With every new request uploading a \textit{DataEntry} the queries 
for fact and dimension tables were computed completely. To avoid
this unnecessary work, the trunks of the queries now get precomputed.


\paragraph{Change detail}
\begin{itemize}
  \item Precompute trunks of upload queries
\end{itemize}


\subsubsection{Generating keys with HashBuilder}
For every dimension the key of a row is needed in the fact table.
Instead of trying to upload the dimension data and get a auto generated key if possible,
otherwise requesting the key by hand, the keys are generated with a
\textit{HashBuilder} now. The builder gets all items of a row and
returns a hashed key, which is used for the upload of the dimension data
and the fact table.

\paragraph{Change detail}
\begin{itemize}
  \item Generating keys with \textit{HashBuilder}
\end{itemize}


\subsubsection{No auto committing}
Every dimension and the fact table rows were committed one by one. Decreasing
the overhead the statement was set to no auto committing. This made it possible
to collect all the upload queries in the statement and send them to the database
all at once.

\paragraph{Change detail} 
\begin{itemize}
  \item Use no auto committing statements for upload
  \item Execute all upload queries of on \textit{DataEntry} at once
\end{itemize}

There was a try to use a second connection pool for the no auto committing connections.
They got set to no auto committing once. But this didn't work out, because it seemed,
that they didn't store this behavior. So it was changed back to the old strategy
changing every used no auto committing connection directly before the use.

\subsection{Extracting}
\texttt{(SELECT * FROM tablename WHERE \ldots) AS nickname} was used first to filter the dimensions.
This was changed to the more easy to build and, as it seemed, faster version with
\texttt{JOIN tablenames ON \ldots}. Making queries got easier because you just have to
list the table names, the key filters to the fact table and the conditions of the dimensions.

\paragraph{Change detail} 
\begin{itemize}
  \item Use \texttt{JOIN} instead of \texttt{SELECT *}
\end{itemize}

As mentioned before (\ref{chart}) the new approach with the chart host being a parameter
of a chart request and the creating of query parts directly in the chart host
made it more easy to build the complex queries. Below is a possible query.

%TODO
TODO codebeispiel

\subsection{Closing}
There showed up problems, when the system was started often without starting the MySQL server new.
The number of connections getting from a MySQL server is limited. With every creation of a \textit{Facade}
with a configuration file there are also created all the mediators, including the \textit{DataMediator}.
This triggers the creation of a connection pool. So when the system is started to often
MySQL will run out of connections and the connection pool won't be created.

Starting the play server to often will face this problem. But this shouldn't happen in a normal
operation mode.

In the JUnit tests the \textit{Facade} is reset with nearly every test. So the problem could be
solved by releasing all connections when resetting the \textit{Facade}. 

\paragraph{Change detail} 
\begin{itemize}
  \item Release connections when resetting \textit{Facade}
\end{itemize}


\subsection{MySQL queries + Testing}
The queries were produced in all conscience. This means it was tested in the 
MySQL workbench how the queries have to look and what would work. Knowing this
the code producing this queries was implemented.

To test whether correct queries are produced they were printed for several requests
and checked by hand if they are of the wanted format.

After uploads of small log files the tables were examined whether they contain all
and the correct data. It was also checked whether the filter options on the web page
contain all the possible data in the tables. For a small amount it was checked
whether the requested data of charts was reasonable. Or for filters for which no 
data exists, whether the data returned is really empty.


\paragraph{Test details}
\begin{itemize}
  \item Print all queries and compare to wanted queries
  \item Compare data in tables to parsed log files
  \item Compare String on web page filters to data in tables 
  \item Compare chart data with requests and tables
\end{itemize}



%\subsection{Connection pool}
%Known problem: No driver there if to often started and not returned. TODO 

