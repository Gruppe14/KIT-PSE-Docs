\section{Data access}

\subsection{Tables}
Instead of creating tables by hand for every configuration file, functions
were added to make it possible to create tables dynamically from
a configuration. This depended on another feature,
storing the configsfor which the tables had already been created.


\paragraph{Change detail}
\begin{itemize}
  \item Add feature to create tables dynamically
  \item Add feature to store and check if tables for configuration are already created
\end{itemize}

There showed up problems with the time and location table

which are always the same. Therefor the creating query

was changed to \texttt{CREATE IF NOT EXITS}.

Below are the creation queries for the SkyServer tables.
\begin{lstlisting}
CREATE TABLE IF NOT EXISTS fact_SkyServer 
	(time_key INT(5), location_key INT(5), server_db_key INT(5),
	 row_elapsed FLOAT,  row_busy FLOAT,
	 row_rows INT(3), type_key INT(5));
CREATE TABLE IF NOT EXISTS dim_time 
	(row_year INT(3), row_month INT(3), row_day INT(3),
	 row_hour INT(3), row_minute INT(3),
	 row_second INT(3), time_key INT(5) UNIQUE);
CREATE TABLE IF NOT EXISTS dim_location 
	(row_country VARCHAR(40), row_city VARCHAR(40),
	 location_key INT(5) UNIQUE);
CREATE TABLE IF NOT EXISTS dim_server_db 
	(row_server VARCHAR(40), row_database VARCHAR(40),
	 server_db_key INT(5) UNIQUE);
CREATE TABLE IF NOT EXISTS dim_type 
	(row_type VARCHAR(40), type_key INT(5) UNIQUE);
\end{lstlisting}

\paragraph{Change detail}
\begin{itemize}
  \item Change table creation query to allow same tables for more configuration
\end{itemize}

%todo, look at
If dimensions of different configurations have the same name - except time and location -
will cause incorrect work. The table names are created from the dimension names.
It was decided to give the dimensions always a new name. If this is
not wanted, it could be changed by concatenating
the configuration name with dimension name and the dimension table short
to create the table name.

\subsection{Uploading}
\subsubsection{Query trunks}
With every new request uploading a \textit{DataEntry} the queries 
for fact and dimension tables were computed completely. To avoid
this unnecessary work, the trunks of the queries now get precomputed.


\paragraph{Change detail}
\begin{itemize}
  \item Precompute trunks of upload queries
\end{itemize}


\subsubsection{Generating keys with HashBuilder}
For every dimension the key of a row is needed in the fact table.
Instead of trying to upload the dimension data and get a auto generated key if possible,
otherwise requesting the key by hand, the keys are generated with a
\textit{HashBuilder} now. The builder gets all items of a row and
returns a hashed key, which is used for the upload of the dimension data
and the fact table.

\paragraph{Change detail}
\begin{itemize}
  \item Generating keys with \textit{HashBuilder}
\end{itemize}


\subsubsection{No auto committing}\label{noAu}
Every dimension and the fact table rows were committed one by one. Decreasing
the overhead the statement was set to no auto committing. This made it possible
to collect all the upload queries in the statement and send them to the database
all at once.

\paragraph{Change detail} 
\begin{itemize}
  \item Use no auto committing statements for upload
  \item Execute all upload queries of on \textit{DataEntry} at once
\end{itemize}

There was a try to use a second connection pool for the no auto committing connections.
They got set to no auto committing once. But this didn't work out, because it seemed,
that they didn't store this behavior. So it was changed back to the old strategy
changing every used no auto committing connection directly before the use.

\pagebreak[4]
\subsection{Extracting}
\texttt{(SELECT * FROM tablename WHERE \ldots) AS nickname} was used first to filter the dimensions.
This was changed to the more easy to build and, as it seemed, faster version with
\texttt{JOIN tablenames ON \ldots}. Making queries got easier because you just have to
list the table names, the key filters to the fact table and the conditions of the dimensions.

\paragraph{Change detail} 
\begin{itemize}
  \item Use \texttt{JOIN} instead of \texttt{SELECT *}
\end{itemize}

As mentioned before (\ref{chart}) the new approach with the chart host being a parameter
of a chart request and the creating of query parts directly in the chart host
made it more easy to build the complex queries. Below is a possible query.

\begin{lstlisting}
SELECT count(*),  row_server 
FROM fact_SkyServer AS  FT  
JOIN  ( dim_server_db AS dim_server_dbID,  
	dim_location AS dim_locationID,  
	dim_type AS dim_typeID )
ON FT.server_db_key = dim_server_dbID.server_db_key
  AND FT.location_key = dim_locationID.location_key
  AND FT.type_key = dim_typeID.type_key
  AND ((dim_server_dbID.row_server = 'DR1_LONG' AND
         ((dim_server_dbID.row_database = 'BESTDR1' ))) 
     OR (dim_server_dbID.row_server = 'DR2_LONG' )
     OR (dim_server_dbID.row_server = 'ROSAT_QUICK' ))
  AND ((dim_locationID.row_country = 'Argentina' AND
         ((dim_locationID.row_city = 'Buenos Aires' )
         OR (dim_locationID.row_city = 'Cordoba' )))   
     OR (dim_locationID.row_country = 'Canada' AND
         ((dim_locationID.row_city = 'Montreal' )
         OR (dim_locationID.row_city = 'Ottawa' )
         OR (dim_locationID.row_city = 'Vancouver' ))))  
  AND ((dim_typeID.row_type = 'OTHER' )  
     OR (dim_typeID.row_type = 'PHOTO' )  
     OR (dim_typeID.row_type = 'QSO' ))  
  AND STR_TO_DATE(CONCAT(CONCAT(CONCAT(CONCAT(
     CONCAT(CAST(row_year AS CHAR),','),
     CONCAT(CAST(row_month AS CHAR),',') ),
     CONCAT(CAST(row_day AS CHAR),',') ),
     CONCAT(CAST(row_hour AS CHAR),',') ),
     CONCAT(CAST(row_minute AS CHAR),',00') ) ,'%Y,%m,%d,%H,%i,%s')  
   BETWEEN STR_TO_DATE('2012,4,7,12,14,00','%Y,%m,%d,%H,%i,%s')  
   AND STR_TO_DATE('2013,2,11,18,19,00','%Y,%m,%d,%H,%i,%s')
GROUP BY row_server
\end{lstlisting}


\subsection{Closing}
There showed up problems, when the system was started often without starting the MySQL server new.
The number of connections getting from a MySQL server is limited. With every creation of a \textit{Facade}
with a configuration file there are also created all the mediators, including the \textit{DataMediator}.
This triggers the creation of a connection pool. So when the system is started to often
MySQL will run out of connections and the connection pool won't be created.

Starting the play server to often will face this problem. This was solved by adding a closing function
executed before the system stops, which releases the \textit{Facade}.

In the JUnit tests the \textit{Facade} is reset with nearly every test. So the problem could be
solved by releasing all connections when resetting the \textit{Facade}. 

\paragraph{Change detail} 
\begin{itemize}
  \item Release connections when resetting \textit{Facade}
   \item Release and reset \textit{Facade} when stoping system
\end{itemize}


\subsection{MySQL queries + Testing}
The queries were produced in all conscience. This means it was tested in the 
MySQL workbench how the queries have to look and what would work. Knowing this
the code producing this queries was implemented.

To test whether correct queries are produced they were printed for several requests
and checked by hand if they are of the wanted format.

After uploads of small log files the tables were examined whether they contain all
and the correct data. It was also checked whether the filter options on the web page
contain all the possible data in the tables. For a small amount it was checked
whether the requested data of charts was reasonable. Or for filters for which no 
data exists, whether the data returned is really empty.


\paragraph{Test details}
\begin{itemize}
  \item Print all queries and compare to wanted queries
  \item Compare data in tables to parsed log files
  \item Compare String on web page filters to data in tables 
  \item Compare chart data with requests and tables
\end{itemize}



%\subsection{Connection pool}
%Known problem: No driver there if to often started and not returned. TODO 

